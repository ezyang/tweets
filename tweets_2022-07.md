CSE is tricky because (1) it requires moving compute around, which can be impeded by side effects, and the moving of the compute itself can result in extra unnecessary work being done, and (2) it can change the resident memory of the program by retaining intermediates. What else?

(Originally on Twitter: [Tue Jul 05 02:43:51 +0000 2022](https://twitter.com/ezyang/status/1544150033888681984))
----
@MarisaVeryMoe (1) I suppose we can distinguish between inlining (when code is pushed to use site) and floating (when code is pulled out to be run earlier). CSE is the floaty type. (2) Ya, absolutely.

(Originally on Twitter: [Tue Jul 05 03:14:29 +0000 2022](https://twitter.com/ezyang/status/1544157741958176768))
----
@peter_a_goodman Parallelism is a side effect!

(Originally on Twitter: [Tue Jul 05 03:15:27 +0000 2022](https://twitter.com/ezyang/status/1544157987119448064))
----
@hazelweakly Yes, though this is universally true for all optimizations and in my experience CSE isn't more affected by this than other things

(Originally on Twitter: [Tue Jul 05 03:16:28 +0000 2022](https://twitter.com/ezyang/status/1544158241369870336))
----
@MarisaVeryMoe I think I'd claim that floating is more difficult to do than inlining. At least it was in GHC anyway lol

(Originally on Twitter: [Tue Jul 05 03:19:27 +0000 2022](https://twitter.com/ezyang/status/1544158993798529024))
----
@MarisaVeryMoe zdevito's hypothesis for years now is we don't need graphs just make everything lazy and have a really low overhead implementation and that's good enough. A lot of one off examples all over, like views, conjugation, linear operators...

(Originally on Twitter: [Tue Jul 05 03:35:35 +0000 2022](https://twitter.com/ezyang/status/1544163054165819392))
----
@trashcanmagic @soumithchintala "PyTorch"

(Originally on Twitter: [Wed Jul 06 00:36:36 +0000 2022](https://twitter.com/ezyang/status/1544480398121500673))
----
@mihaimaruseac With this sort of thing, it seems best to just give up on GitHub's internal tooling and roll it yourself haha. If you have a merge bot you're most of the way there, just need to setup the reviewer bot

(Originally on Twitter: [Thu Jul 07 02:36:02 +0000 2022](https://twitter.com/ezyang/status/1544872842013151235))
----
@mihaimaruseac At least GHA has made this a lot easier

(Originally on Twitter: [Thu Jul 07 04:40:35 +0000 2022](https://twitter.com/ezyang/status/1544904183823867904))
----
Which is more difficult: writing an ahead-of-time (symbolic) or runtime (eager) reverse mode automatic differentiation implementation? For the PyTorch team, you can observe that we struggled with AoT more. But it is difficult to suss out exactly why this is the case. ðŸ§µ

(Originally on Twitter: [Sun Jul 10 04:15:46 +0000 2022](https://twitter.com/ezyang/status/1545985105088462849))
----
Explanation 3: we're doing it wrong. JAX says right way to write an AoT AD system is to write an eager forward AD system and a symbolic transposer. Suuuper simple. https://arxiv.org/pdf/2105.09469.pdf Decomposing is a good idea and there's other ways to do it, e.g. https://richarde.dev/papers/2022/ad/higher-order-ad.pdf

(Originally on Twitter: [Sun Jul 10 04:15:47 +0000 2022](https://twitter.com/ezyang/status/1545985109446332416))
----
(Something that I find completely inexplicable but is true in practice is in multiple domains--quantization, distributed--it seems way easier for devs to implement functionality AoT rather than eager mode. But somehow not AD?)

(Originally on Twitter: [Sun Jul 10 04:15:47 +0000 2022](https://twitter.com/ezyang/status/1545985108397662208))
----
Explanation 2: symbolic is more complicated than eager. This doesn't have to be true: you can turn eager AD into symbolic AD by tracing through it (AOTAutograd). But if you're writing an AoT system there's a big temptation to support control flow language features.

(Originally on Twitter: [Sun Jul 10 04:15:47 +0000 2022](https://twitter.com/ezyang/status/1545985107357573121))
----
Explanation 1: it's a historical artifact: eager autograd came first, TorchScript symbolic autograd came after. If most of the user/dev mass is on eager autograd, of course you're going to have a harder time keeping the symbolic autodiff up to date.

(Originally on Twitter: [Sun Jul 10 04:15:47 +0000 2022](https://twitter.com/ezyang/status/1545985106313203714))
----
The true explanation is probably some combination of all of these factors. It's just interesting how sensitive "the right design" can be to all sorts of things that typically don't show up at all in the technical requirements.

(Originally on Twitter: [Sun Jul 10 04:15:48 +0000 2022](https://twitter.com/ezyang/status/1545985112663379969))
----
Explanation 5: it is a matter of background. People who have spent all their time writing eager mode transforms find this easier. Compiler nuts who spend all their time writing AoT passes find that easier. It's all contingent on who the people are.

(Originally on Twitter: [Sun Jul 10 04:15:48 +0000 2022](https://twitter.com/ezyang/status/1545985111614693377))
----
Explanation 4: eager autograd is actually really hard but some smart people implemented it correctly way back in the day, and if the apocalypse happened and we had to rebuild it from scratch we'd be in for a hard time.

(Originally on Twitter: [Sun Jul 10 04:15:48 +0000 2022](https://twitter.com/ezyang/status/1545985110574600195))
----
@MarisaVeryMoe If I write a Turing machine interpreter by translating it to lambda calculus first, have I really written a TM interpreter? If I write symbolic AD by staging a runtime AD, have I really written a symbolic AD?

(Originally on Twitter: [Sun Jul 10 19:39:40 +0000 2022](https://twitter.com/ezyang/status/1546217610265739264))
----
@yaroslavvb Yeah, that it took so long for people to realize AD was possible is a strong indication that it would be hard to reinvent if forgotten

(Originally on Twitter: [Sun Jul 10 20:19:31 +0000 2022](https://twitter.com/ezyang/status/1546227637714145283))
----
def f(a): return g1(a), g2(a). Fusing g1 and g2 together is:

(Originally on Twitter: [Fri Jul 15 03:12:58 +0000 2022](https://twitter.com/ezyang/status/1547781240257777664))
----
Follow up from @cHHillee: def f(a, b): return g(a) + g(b). Fusing g1 and g2 together is:

(Originally on Twitter: [Fri Jul 15 03:28:21 +0000 2022](https://twitter.com/ezyang/status/1547785108794396672))
----
For bonus points, tell us what your preferred, unambiguous nomenclature for these fusion patterns are.

(Originally on Twitter: [Fri Jul 15 03:34:38 +0000 2022](https://twitter.com/ezyang/status/1547786691657617408))
----
so you're like "oh maybe cuda initialization is slow but what if I get make a cuda initialized daemon and then fork off subsequent process calls" but noooooo it has to be multithreaded so of course that doesn't work

(Originally on Twitter: [Sun Jul 17 03:11:25 +0000 2022](https://twitter.com/ezyang/status/1548505626325569538))
----
@jamesr66a Do you remember how much you paid ðŸ˜‚

(Originally on Twitter: [Sun Jul 17 22:44:20 +0000 2022](https://twitter.com/ezyang/status/1548800798070538240))
----
How much of CUDA programming's reputation for being hard comes out of people writing kernels from scratch only when they need performance and end up overoptimizing the kernels for what they need.

(Originally on Twitter: [Thu Jul 21 04:55:05 +0000 2022](https://twitter.com/ezyang/status/1549981264467329024))
----
Would you trust copilot to write bit twiddling C/C++ code from English description

(Originally on Twitter: [Thu Jul 28 21:48:16 +0000 2022](https://twitter.com/ezyang/status/1552772954626351108))
----
@francoisfleuret @PyTorch we done goofed

(Originally on Twitter: [Thu Jul 28 22:55:29 +0000 2022](https://twitter.com/ezyang/status/1552789871634382849))
----
The horrible thing about portability is C/C++ have this beautiful syntax for manipulating bits AND YOU CAN'T USE IT ðŸ˜¤

(Originally on Twitter: [Fri Jul 29 00:59:50 +0000 2022](https://twitter.com/ezyang/status/1552821165256761345))
----
@krismicinski @terrible_coder I'm reading through the Stack Overflow answers and they are all garbage ðŸ¤¬

(Originally on Twitter: [Fri Jul 29 03:23:25 +0000 2022](https://twitter.com/ezyang/status/1552857301295534082))
----
The problem with undefined behavior is that in practice you need to know the typical behavior of UB so you can tell if it is happening or not

(Originally on Twitter: [Sun Jul 31 02:51:31 +0000 2022](https://twitter.com/ezyang/status/1553574047530852357))
----
Turns out, C++ also has the orphan instances problem, but the compiler doesn't even bother trying to find them and will happily commit an ODR violation if you get it wrong. Lol. https://github.com/pytorch/pytorch/pull/82552

(Originally on Twitter: [Sun Jul 31 19:28:42 +0000 2022](https://twitter.com/ezyang/status/1553824995150860290))
----
